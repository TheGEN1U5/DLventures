{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zgcElr6a8UJr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3XckVYQA8g4x"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "res = requests.get('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
        "training_text = res.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9zQ-osxS9AmA"
      },
      "outputs": [],
      "source": [
        "training_text = training_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTEAjfA-9R4k",
        "outputId": "157aa2ef-c3a9-4544-da8e-a73d118dadf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "202651"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oVZu9rADDfW4"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Metaspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "A-bWo0u3Lo3H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tkz = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tkz.pre_tokenizer = Metaspace(replacement=\"▁\")\n",
        "trainer = BpeTrainer(vocab_size=10000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tkz.train_from_iterator([training_text], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9juR_1ojJU44"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fX6TGhwbKnh9"
      },
      "outputs": [],
      "source": [
        "output = tkz.encode(training_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QubgXVWQMAyI"
      },
      "outputs": [],
      "source": [
        "training_tokens = output.ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ocD-y4aJRW5X"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokens, seq_len):\n",
        "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.tokens[idx : idx + self.seq_len]\n",
        "        y = self.tokens[idx + 1 : idx + self.seq_len + 1]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEPyzIP3CCZ5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LJgbl6lnRW7j"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(TextDataset(training_tokens, 69), batch_size=500, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyJhnr78RW-y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UnDQqTkdKI3Q"
      },
      "outputs": [],
      "source": [
        "class generator_nn(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_size):\n",
        "        super(generator_nn, self).__init__()\n",
        "        self.s_i = input_size\n",
        "        self.s_h = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_layer = nn.Embedding(vocab_size, input_size, padding_idx=1)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "    def forward(self, x):\n",
        "        vec = self.embed_layer(x)\n",
        "        o, (hn, cn) = self.lstm(vec)\n",
        "        logits = self.linear(o)\n",
        "        return logits\n",
        "\n",
        "def train(model, training_loader, lr, epochs, threshold=0.5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    losses = np.zeros(epochs)\n",
        "    # val_losses = np.zeros(epochs)\n",
        "    # val_f1 = np.zeros(epochs)\n",
        "    for e in tqdm(range(epochs), leave=False):\n",
        "        model.train()\n",
        "        loss_epoch = 0\n",
        "        for X_batch, y_batch in training_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X_batch)\n",
        "            # print(logits.shape, y_batch.shape)\n",
        "            loss = loss_fn(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_epoch += loss.item() * X_batch.shape[0]\n",
        "        losses[e] = loss_epoch / len(training_loader.dataset)\n",
        "\n",
        "        # if val_loader:\n",
        "        #     val_losses[e], val_f1[e] = validation_metrics(model, val_loader, loss_fn, threshold, device)\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "48CzHoC1RWSv",
        "outputId": "4cce559b-853d-4c44-b6e8-0afb9b0574be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([7.82515433, 7.45389622, 7.23678058, 6.99901495, 6.77451421,\n",
              "       6.56115203, 6.36136525, 6.17958904, 6.0128832 , 5.85836311,\n",
              "       5.7158521 , 5.58382633, 5.4610609 , 5.34640759, 5.23871503,\n",
              "       5.13693306, 5.04018636, 4.94772002, 4.85895882, 4.77355869,\n",
              "       4.69122288, 4.61164621, 4.53449751, 4.45962562, 4.38678423,\n",
              "       4.31580745, 4.24660757, 4.17898392, 4.11292127, 4.04825614,\n",
              "       3.98489208, 3.9227714 , 3.86182134, 3.80199781, 3.74322847,\n",
              "       3.68547478, 3.62869254, 3.57287734, 3.5180204 , 3.46409309,\n",
              "       3.4110785 , 3.35892864, 3.30765712, 3.25725251, 3.2076316 ,\n",
              "       3.15885515, 3.11085133, 3.06365805, 3.01721776, 2.97154609])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator = generator_nn(100, 256, tkz.get_vocab_size())\n",
        "train(generator, train_loader, lr=0.0001, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator.to(\"cpu\")\n",
        "torch.save(generator, \"/teamspace/studios/this_studio/DLventures/gen.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNwuj7nC12wd"
      },
      "outputs": [],
      "source": [
        "a = np.random.rand(2,3,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XuJ6mqVAMj6"
      },
      "outputs": [],
      "source": [
        "a.reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "bmWlHuoCALHZ",
        "outputId": "92216189-2a89-4622-9beb-bab18998160f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 400/400 [01:27<00:00,  4.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss = 7.1092\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  21%|██▏       | 85/400 [00:18<01:09,  4.54it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3401723783>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}: Loss = {total_loss / len(loader):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, length=200_000, seq_len=69):\n",
        "        self.data = torch.randint(0, 10_000, (length,), dtype=torch.long)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx : idx + self.seq_len]\n",
        "        y = self.data[idx + 1 : idx + self.seq_len + 1]\n",
        "        return x, y\n",
        "\n",
        "class ToyModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=100, hidden_size=256):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=1)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Hyperparams\n",
        "vocab_size = 10_000\n",
        "batch_size = 500\n",
        "seq_len = 69\n",
        "epochs = 3\n",
        "\n",
        "# Data\n",
        "dataset = ToyDataset()\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "# Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ToyModel(vocab_size).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
        "    for xb, yb in pbar:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = loss_fn(out.reshape(-1, vocab_size), yb.reshape(-1))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch}: Loss = {total_loss / len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgYus_6Qjwqt",
        "outputId": "4fdb1c18-4699-4973-8448-4147e65463fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n",
            "12.4\n",
            "90300\n",
            "Tesla T4\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "print(torch.backends.cudnn.version())\n",
        "print(torch.cuda.get_device_name())\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEKp6ogPkmME"
      },
      "outputs": [],
      "source": [
        "# clear torch cahe\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orQAWUsSkrQc"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yLmU3bxkvcU"
      },
      "outputs": [],
      "source": [
        "del generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyD3_9Ptk1Qr",
        "outputId": "90eed2de-43ca-42d1-aee3-ef5b32f5e53c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500 LSTM forward passes took 4.96 seconds\n"
          ]
        }
      ],
      "source": [
        "import torch, time\n",
        "x = torch.randn(500, 69, 100).cuda()\n",
        "lstm = torch.nn.LSTM(100, 256, batch_first=True).cuda()\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(500):\n",
        "    out, _ = lstm(x)\n",
        "torch.cuda.synchronize()\n",
        "print(f\"500 LSTM forward passes took {time.time() - start:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pMt3kHmlClQ",
        "outputId": "eb09b030-29b0-4226-e55f-b286f0e2bf11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Jun 12 19:19:37 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0             29W /   70W |    3220MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cOloerKlUiz"
      },
      "outputs": [],
      "source": [
        "del x\n",
        "del lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9KXHTfglppi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
